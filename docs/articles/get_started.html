<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Motivation, Installation and Quick Workflow • FSelectorRcpp</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Motivation, Installation and Quick Workflow">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">FSelectorRcpp</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">0.3.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/benchmarks_discretize.html">Benchmarks: discretize()</a>
    </li>
    <li>
      <a href="../articles/get_started.html">Motivation, Installation and Quick Workflow</a>
    </li>
    <li>
      <a href="../articles/integer-variables.html">Integer variables.</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Motivation, Installation and Quick Workflow</h1>
            <h3 class="subtitle">
<small> <a href="http://r-addict.com/About.html">Marcin Kosinski</a>   <a href="https://stackoverflow.com/users/3857701"><i class="fa fa-stack-overflow"></i></a>  <a href="http://r-addict.com"><i class="fa fa-comment"></i></a>  <a href="https://github.com/MarcinKosinski"><i class="fa fa-github"></i></a>  <a href="mailto:m.p.kosinski@gmail.com"><i class="fa fa-envelope-o"></i></a> <br><a href="http://www.zstat.pl/">Zygmunt Zawadzki</a>   <a href="http://www.zstat.pl/"><i class="fa fa-comment"></i></a>  <a href="https://github.com/zzawadz"><i class="fa fa-github"></i></a>  <a href="mailto:zygmunt@zstat.pl"><i class="fa fa-envelope-o"></i></a></small><br>
</h3>
            
      
      
      <div class="hidden name"><code>get_started.Rmd</code></div>

    </div>

    
    
<p><a href="http://mi2-warsaw.github.io/FSelectorRcpp/">FSelectorRcpp</a> is an <a href="https://CRAN.R-project.org/package=Rcpp">Rcpp</a> (free of Java/Weka) implementation of <a href="https://CRAN.R-project.org/package=FSelector">FSelector</a> entropy-based feature selection algorithms with a sparse matrix support. It is also equipped with a parallel backend.</p>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">install.packages</span>(<span class="st">'FSelectorRcpp'</span>) <span class="co"># stable release version on CRAN</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2">devtools<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/devtools/topics/reexports">install_github</a></span>(<span class="st">'mi2-warsaw/FSelectorRcpp'</span>) <span class="co"># dev version</span></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="co"># windows users should have Rtools for devtools installation</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="co"># https://cran.r-project.org/bin/windows/Rtools/</span></a></code></pre></div>
</div>
<div id="motivation" class="section level2">
<h2 class="hasAnchor">
<a href="#motivation" class="anchor"></a>Motivation</h2>
<p>In the modern statistical learning the biggest bottlenecks are computation times of model training procedures and the overfitting. Both are caused by the same issue - the high dimension of explanatory variables space. Researchers have encountered problems with too big sets of features used in machine learning algorithms also in terms of model interpretaion. This motivates applying feature selection algorithms before performing statisical modeling, so that on smaller set of attributes the training time will be shorter, the interpretation might be clearer and the noise from non important features can be avoided. More motivation can be found in <span class="citation">(John, Kohavi, and Pfleger 1994)</span>.</p>
<p>Many methods were developed to reduce the curse of dimensionality like <strong>Principal Component Analysis</strong> <span class="citation">(Pearson 1901)</span> or <strong>Singular Value Decomposition</strong> <span class="citation">(Eckart and Young 1936)</span> which approximates the variables by smaller number of combinations of original variables, but this approach is hard to interpret in the final model.</p>
<p>Sophisticated methods of attribute selection as <strong>Boruta</strong> algoritm <span class="citation">(Kursa and Rudnicki 2010)</span>, genetic algorithms <span class="citation">(Kuhn and Johnson 2013, <span class="citation">@FedCSIS2013l106</span>)</span> or simulated annealing techniques <span class="citation">(Khachaturyan, Semenovsovskaya, and Vainshtein 1981)</span> are known and broadly used but in some cases for those algorithms computations can take even days, not to mention that datasets are growing every hour.</p>
<p>Few classification and regression models can reduce redundand variables during the training phase of statistical learning process, e.g. <strong>Decision Trees</strong> <span class="citation">(Rokach and Maimon 2008, <span class="citation">@cart84</span>)</span>, <strong>LASSO Regularized Generalized Linear Models</strong> (with cross-validation) <span class="citation">(Friedman, Hastie, and Tibshirani 2010)</span> or <strong>Regularized Support Vector Machine</strong> <span class="citation">(Xu, Caramanis, and Mannor 2009)</span>, but still computations starting with full set of explanatory variables are time consuming and the understaning of the feature selection procedure in this case is not simple and those methods are sometimes used without the understanding.</p>
<p>In business applications there appear a need to provide a fast feature selection that is extremely easy to understand. For such demands easy methods are prefered. This motivates using simple techniques like <strong>Entropy Based Feature Selection</strong> <span class="citation">(Largeron, Moulin, and Géry 2011)</span>, where every feature can be checked independently so that computations can be performed in a parallel to shorter the procedure’s time. For this approach we provide an R interface to <strong>Rcpp</strong> reimplementation <span class="citation">(Dirk Eddelbuettel 2011)</span> of methods included in <strong>FSelector</strong> package which we also extended with parallel background and sparse matrix support. This has significant impact on computations time and can be used on greater datasets, comparing to <strong>FSelector</strong>. Additionally we avoided the Weka <span class="citation">(Hall et al. 2009)</span> dependency and we provided faster discretization implementations than those from <strong>entropy</strong> package, used originally in <strong>FSelector</strong>.</p>
</div>
<div id="quick-workflow" class="section level2">
<h2 class="hasAnchor">
<a href="#quick-workflow" class="anchor"></a>Quick Workflow</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">library</span>(magrittr)</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="kw">library</span>(FSelectorRcpp)</a></code></pre></div>
<p>A simple entropy based feature selection workflow. <strong>Information gain</strong> is an easy, linear algorithm that computes the entropy of a dependent and explanatory variables, and the conditional entropy of a dependent variable with a respect to each explanatory variable separately. This simple statistic enables to calculate the belief of the distribution of a dependent variable when we only know the distribution of a explanatory variable.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw"><a href="../reference/information_gain.html">information_gain</a></span>(               <span class="co"># Calculate the score for each attribute</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">    <span class="dt">formula =</span> Species <span class="op">~</span><span class="st"> </span>.,      <span class="co"># that is on the right side of the formula.</span></a>
<a class="sourceLine" id="cb3-3" data-line-number="3">    <span class="dt">data =</span> iris,                <span class="co"># Attributes must exist in the passed data.</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4">    <span class="dt">type  =</span> <span class="st">"infogain"</span>,         <span class="co"># Choose the type of a score to be calculated.</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5">    <span class="dt">threads =</span> <span class="dv">2</span>                 <span class="co"># Set number of threads in a parallel backend.</span></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">  ) <span class="op">%&gt;%</span><span class="st">                          </span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7"><span class="st">  </span><span class="kw"><a href="../reference/cut_attrs.html">cut_attrs</a></span>(                    <span class="co"># Then take attributes with the highest rank.</span></a>
<a class="sourceLine" id="cb3-8" data-line-number="8">    <span class="dt">k =</span> <span class="dv">2</span>                       <span class="co"># For example: 2 attrs with the higehst rank.</span></a>
<a class="sourceLine" id="cb3-9" data-line-number="9">  ) <span class="op">%&gt;%</span><span class="st">                         </span></a>
<a class="sourceLine" id="cb3-10" data-line-number="10"><span class="st">  </span><span class="kw"><a href="../reference/to_formula.html">to_formula</a></span>(                   <span class="co"># Create a new formula object with </span></a>
<a class="sourceLine" id="cb3-11" data-line-number="11">    <span class="dt">attrs =</span> .,                  <span class="co"># the most influencial attrs.</span></a>
<a class="sourceLine" id="cb3-12" data-line-number="12">    <span class="dt">class =</span> <span class="st">"Species"</span>           </a>
<a class="sourceLine" id="cb3-13" data-line-number="13">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb3-14" data-line-number="14"><span class="st">  </span><span class="kw">glm</span>(</a>
<a class="sourceLine" id="cb3-15" data-line-number="15">    <span class="dt">formula =</span> .,                <span class="co"># Use that formula in any classification algorithm.</span></a>
<a class="sourceLine" id="cb3-16" data-line-number="16">    <span class="dt">data =</span> iris,                </a>
<a class="sourceLine" id="cb3-17" data-line-number="17">    <span class="dt">family =</span> <span class="st">"binomial"</span>         </a>
<a class="sourceLine" id="cb3-18" data-line-number="18">  )</a></code></pre></div>
<pre><code>
Call:  glm(formula = ., family = "binomial", data = iris)

Coefficients:
 (Intercept)   Petal.Width  Petal.Length  
      -69.45         33.89         17.60  

Degrees of Freedom: 149 Total (i.e. Null);  147 Residual
Null Deviance:      191 
Residual Deviance: 5.17e-09     AIC: 6</code></pre>
<p>Apply a complicated feature selection search engine, that checks combinations of subsets of the specified attributes’ set, to score the currently considered subset, depending on the criterion passed with the <code>fun</code> parameter.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">evaluator_R2_lm &lt;-<span class="st">    </span><span class="co"># Create a scorer function.</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="st">  </span><span class="cf">function</span>(</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">    attributes,       <span class="co"># That takes the currently considered subset of attributes</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4">    data,             <span class="co"># from a specified dataset.</span></a>
<a class="sourceLine" id="cb5-5" data-line-number="5">    <span class="dt">dependent =</span> </a>
<a class="sourceLine" id="cb5-6" data-line-number="6">      <span class="kw">names</span>(data)[<span class="dv">1</span>]  <span class="co"># To find features that best describe the dependent variable.</span></a>
<a class="sourceLine" id="cb5-7" data-line-number="7">  ) {</a>
<a class="sourceLine" id="cb5-8" data-line-number="8">    <span class="kw">summary</span>(          <span class="co"># In this situation we take the r.squared statistic</span></a>
<a class="sourceLine" id="cb5-9" data-line-number="9">      <span class="kw">lm</span>(             <span class="co"># from the summary of a linear model object.</span></a>
<a class="sourceLine" id="cb5-10" data-line-number="10">        <span class="kw"><a href="../reference/to_formula.html">to_formula</a></span>(   <span class="co"># This is the score to use to choose between considered </span></a>
<a class="sourceLine" id="cb5-11" data-line-number="11">          attributes, <span class="co"># subsets of attributes.</span></a>
<a class="sourceLine" id="cb5-12" data-line-number="12">          dependent</a>
<a class="sourceLine" id="cb5-13" data-line-number="13">        ),</a>
<a class="sourceLine" id="cb5-14" data-line-number="14">        <span class="dt">data =</span> data)</a>
<a class="sourceLine" id="cb5-15" data-line-number="15">    )<span class="op">$</span>r.squared</a>
<a class="sourceLine" id="cb5-16" data-line-number="16">  }</a>
<a class="sourceLine" id="cb5-17" data-line-number="17"></a>
<a class="sourceLine" id="cb5-18" data-line-number="18"><span class="kw"><a href="../reference/feature_search.html">feature_search</a></span>(          <span class="co"># feature_search work in 2 modes - 'exhaustive' and 'greedy'</span></a>
<a class="sourceLine" id="cb5-19" data-line-number="19">  <span class="dt">attributes =</span> </a>
<a class="sourceLine" id="cb5-20" data-line-number="20">    <span class="kw">names</span>(iris)[<span class="op">-</span><span class="dv">1</span>],     <span class="co"># It takes attribues and creates combinations of it's subsets.</span></a>
<a class="sourceLine" id="cb5-21" data-line-number="21">  <span class="dt">fun =</span> evaluator_R2_lm, <span class="co"># And it calculates the score of a subset that depends on the </span></a>
<a class="sourceLine" id="cb5-22" data-line-number="22">  <span class="dt">data =</span> iris,           <span class="co"># evaluator function passed in the `fun` parameter.</span></a>
<a class="sourceLine" id="cb5-23" data-line-number="23">  <span class="dt">mode =</span> <span class="st">"exhaustive"</span>,   <span class="co"># exhaustive - means to check all possible </span></a>
<a class="sourceLine" id="cb5-24" data-line-number="24">  <span class="dt">sizes =</span>                <span class="co"># attributes' subset combinations </span></a>
<a class="sourceLine" id="cb5-25" data-line-number="25">    <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(attributes) <span class="co"># of sizes passed in sizes.</span></a>
<a class="sourceLine" id="cb5-26" data-line-number="26">)<span class="op">$</span>all</a></code></pre></div>
<pre><code>  Sepal.Width Petal.Length Petal.Width Species     values
1           1            0           0       0 0.01382265
2           0            1           0       0  0.7599546
3           0            0           1       0  0.6690277
4           0            0           0       1  0.6187057</code></pre>
</div>
<div id="use-cases" class="section level2">
<h2 class="hasAnchor">
<a href="#use-cases" class="anchor"></a>Use cases</h2>
<ul>
<li><a href="http://r-addict.com/2017/01/08/Entropy-Based-Image-Binarization.html">Entropy Based Image Binarization with imager and FSelectorRcpp</a></li>
<li><a href="http://www.r-bloggers.com/venn-diagram-comparison-of-boruta-fselectorrcpp-and-glmnet-algorithms/">Venn Diagram Comparison of Boruta, FSelectorRcpp and GLMnet Algorithms</a></li>
</ul>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-FedCSIS2013l106">
<p>Aziz, Amira Sayed A., Ahmad Taher Azar, Mostafa A. Salama, Aboul Ella Hassanien, and Sanaa El Ola Hanfy. 2013. “Genetic Algorithms with Different Feature Selection Techniques for Anomaly Detectors Generation.” In <em>Proceedings of the 2013 Federated Conference on Computer Science and Information Systems</em>, edited by M. Paprzycki M. Ganzha L. Maciaszek, pages 769–774. IEEE.</p>
</div>
<div id="ref-cart84">
<p>Breiman, L., J. Friedman, R. Olshen, and C. Stone. 1984. <em>Classification and Regression Trees</em>. Monterey, CA: Wadsworth; Brooks.</p>
</div>
<div id="ref-Rcpp">
<p>Dirk Eddelbuettel, Romain Franccois. 2011. “Rcpp: Seamless R and C++ Integration.” <em>Journal of Statistical Software</em> 40 (8): 1–18. <a href="http://www.jstatsoft.org/v40/i08/" class="uri">http://www.jstatsoft.org/v40/i08/</a>.</p>
</div>
<div id="ref-eckart1936approximation">
<p>Eckart, C., and G. Young. 1936. “The Approximation of One Matrix by Another of Lower Rank.” <em>Psychometrika</em> 1 (3). Springer: 211–18. <a href="https://doi.org/10.1007/BF02288367" class="uri">https://doi.org/10.1007/BF02288367</a>.</p>
</div>
<div id="ref-glmnet">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” <em>Journal of Statistical Software</em> 33 (1): 1–22. <a href="http://www.jstatsoft.org/v33/i01/" class="uri">http://www.jstatsoft.org/v33/i01/</a>.</p>
</div>
<div id="ref-Hall:2009:WDM:1656274-1656278">
<p>Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. “The Weka Data Mining Software: An Update.” <em>SIGKDD Explor. Newsl.</em> 11 (1). New York, NY, USA: ACM: 10–18. <a href="https://doi.org/10.1145/1656274.1656278" class="uri">https://doi.org/10.1145/1656274.1656278</a>.</p>
</div>
<div id="ref-John94irrelevantfeatures">
<p>John, George H., Ron Kohavi, and Karl Pfleger. 1994. “Irrelevant Features and the Subset Selection Problem.” In <em>MACHINE Learning: PROCEEDINGS of the Eleventh International</em>, 121–29. Morgan Kaufmann.</p>
</div>
<div id="ref-Khachaturyan:a19748">
<p>Khachaturyan, A., S. Semenovsovskaya, and B. Vainshtein. 1981. “The thermodynamic approach to the structure analysis of crystals.” <em>Acta Crystallographica Section A</em> 37 (5): 742–54.</p>
</div>
<div id="ref-geneticAlgo">
<p>Kuhn, Max, and Kjell Johnson. 2013. “Applied Predictive Modeling.” New York, NY: Springer. 2013. <a href="http://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/" class="uri">http://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/</a>.</p>
</div>
<div id="ref-Boruta">
<p>Kursa, Miron B., and Witold R. Rudnicki. 2010. “Feature Selection with the Boruta Package.” <em>Journal of Statistical Software</em> 36 (11): 1–13. <a href="http://www.jstatsoft.org/v36/i11/" class="uri">http://www.jstatsoft.org/v36/i11/</a>.</p>
</div>
<div id="ref-Largeron:2011:EBF:1982185-1982389">
<p>Largeron, Christine, Christophe Moulin, and Mathias Géry. 2011. “Entropy Based Feature Selection for Text Categorization.” In <em>Proceedings of the 2011 Acm Symposium on Applied Computing</em>, 924–28. SAC ’11. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/1982185.1982389" class="uri">https://doi.org/10.1145/1982185.1982389</a>.</p>
</div>
<div id="ref-PCA:14786440109462720">
<p>Pearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” <em>Philosophical Magazine Series 6</em> 2 (11): 559–72.</p>
</div>
<div id="ref-Rokach:2008:DMD:1796114">
<p>Rokach, Lior, and Oded Maimon. 2008. <em>Data Mining with Decision Trees: Theroy and Applications</em>. River Edge, NJ, USA: World Scientific Publishing Co., Inc.</p>
</div>
<div id="ref-Xu:2009:RRS:1577069-1755834">
<p>Xu, Huan, Constantine Caramanis, and Shie Mannor. 2009. “Robustness and Regularization of Support Vector Machines.” <em>J. Mach. Learn. Res.</em> 10 (December). JMLR.org: 1485–1510. <a href="http://dl.acm.org/citation.cfm?id=1577069.1755834" class="uri">http://dl.acm.org/citation.cfm?id=1577069.1755834</a>.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#installation">Installation</a></li>
      <li><a href="#motivation">Motivation</a></li>
      <li><a href="#quick-workflow">Quick Workflow</a></li>
      <li><a href="#use-cases">Use cases</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Zygmunt Zawadzki, Marcin Kosinski.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
