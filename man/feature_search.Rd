% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/param_search.R
\name{feature_search}
\alias{feature_search}
\title{General Feature Searching Function}
\usage{
feature_search(attributes, fun, data, mode = c("greedy", "exhaustive"),
  type = c("forward", "backward"), sizes = 1:length(attributes),
  parallel = TRUE, ...)
}
\arguments{
\item{attributes}{A character vector with attributes' names to be used to extract the most valuable features.}

\item{fun}{A function (evaluator) to be used to score features' sets at each iteration of the algorithm passed via \code{mode}.
See Examples.}

\item{data}{A data set for \code{fun} function (evaluator).}

\item{mode}{A character that determines which search algorithm to perform.}

\item{type}{An argument for \code{mode = "greedy"} - whether to use the
\code{backward} or the \code{forward} multiple-way search.}

\item{sizes}{An argument for \code{mode = "exhaustive"}. Vector of sizes
of attributes subsets.}

\item{parallel}{Allow parallelization.}

\item{\dots}{Other arguments passed to \link{foreach} function.}
}
\description{
A convenience wrapper for \code{greedy} and \code{exhaustive} feature selection algorithms that
extract valuable attributes depending on the evaluation method (called evaluator).
}
\examples{

# Enable parallelization in examples
 library(doSNOW) # doSNOW has an option for progress bar
 cl <- makeCluster(2)
 registerDoSNOW(cl)

# Close at the end
# stopCluster(cl)
# registerDoSEQ()

# 1) Evaluator from FSelector package.
evaluator <- function(subset, data, dependent = names(iris)[5]) {
  library(rpart)
  k <- 5
  splits <- runif(nrow(data))
  results <- sapply(1:k, function(i) {
    test.idx <- (splits >= (i - 1) / k) & (splits < i / k)
    train.idx <- !test.idx
    test <- data[test.idx, , drop = FALSE]
    train <- data[train.idx, , drop = FALSE]
    tree <- rpart(to_formula(subset, dependent), train)
    error.rate <- sum(test[[dependent]] != predict(tree, test, type = "c")) /
    nrow(test)
    return(1 - error.rate)
  })
  return(mean(results))
}

# Default greedy search.
system.time(
  feature_search(attributes = names(iris)[-5],
                 fun = evaluator,
                 data = iris)
)
system.time(
  feature_search(attributes = names(iris)[-5],
                 fun = evaluator,
                 data = iris,
                 parallel = FALSE)
)

# Optional exhaustive search.
system.time(
  feature_search(attributes = names(iris)[-5],
                 fun = evaluator,
                 data = iris,
                 mode = "exhaustive")
)
system.time(
  feature_search(attributes = names(iris)[-5],
                 fun = evaluator,
                 data = iris,
                 mode = "exhaustive",
                 parallel = FALSE)
)

# 2) Maximize R^2 statistics in the linear regression model/problem.

evaluator_R2_lm <- function(attributes, data, dependent = names(iris)[1]) {
  summary(
    lm(to_formula(attributes, dependent), data = data)
  )$r.squared
}

feature_search(attributes = names(iris)[-1],
               fun = evaluator_R2_lm, data = iris,
               mode = "exhaustive")

# 3) Optimize BIC crietion in generalized linear model.
# Aim of Bayesian approach it to identify the model with the highest
# probability of being the true model. - Kuha 2004

utils::data(anorexia, package = "MASS")

evaluator_BIC_glm <- function(attributes, data, dependent = "Postwt") {
  extractAIC(
    fit = glm(to_formula(attributes, dependent), family = gaussian,
              data = data),
    k = log(nrow(data))
  )[2]
}

feature_search(attributes = c("Prewt", "Treat", "offset(Prewt)"),
               fun = evaluator_BIC_glm,
               data = anorexia,
               mode = "exhaustive")

# Close parallelization
stopCluster(cl)
registerDoSEQ()

}
\author{
Zygmunt Zawadzki \email{zygmunt.zawadzki@gmail.com}

Krzysztof Slomczynski \email{krzysztofslomczynski@gmail.com}
}

